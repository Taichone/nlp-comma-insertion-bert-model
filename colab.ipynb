{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqnQkmL1X8HJ"
      },
      "source": [
        "# 初回のみ: Mecab と辞書を Google Drive に配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEuIoqQzDTzy"
      },
      "outputs": [],
      "source": [
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "\n",
        "# シンボリックリンクによるエラー回避\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc\n",
        "\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "# /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hA_4LdfDVTG"
      },
      "outputs": [],
      "source": [
        "# Google Drive に NEologd を格納する場合\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "# /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
        "\n",
        "# 「Colab Notebooks」の間にある空白は\\ (backslash)でエスケープ\n",
        "!cp -r /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd /content/drive/MyDrive/Colab\\ Notebooks/Libs/pylibs\n",
        "# /etc/mecabrcが辞書データ参照時に必要なのでetcフォルダごとコピー↓\n",
        "!cp -r /etc /content/drive/MyDrive/Colab\\ Notebooks/Libs/pylibs/etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvj1vwDItZgd"
      },
      "outputs": [],
      "source": [
        "# ローカルランタイムの場合\n",
        "!pip install mecab-python3 > /dev/null\n",
        "\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "import MeCab\n",
        "path2dic = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
        "mecab = MeCab.Tagger(\"-d {0}\".format(path2dic))\n",
        "\n",
        "print(mecab.parse(\"これは自動で読点を挿入する研究です\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy6saF4Qg08H"
      },
      "source": [
        "# MeCab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BgNFpJl7URZ"
      },
      "outputs": [],
      "source": [
        "# Google Drive に NEologd を入れている場合\n",
        "!pip install mecab-python3 > /dev/null\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"MECABRC\"] = '/content/drive/MyDrive/Colab Notebooks/Libs/pylibs/etc/mecabrc'\n",
        "\n",
        "import MeCab\n",
        "path2dic = \"/content/drive/MyDrive/Colab\\ Notebooks/Libs/pylibs/mecab-ipadic-neologd\"\n",
        "mecab = MeCab.Tagger(\"-d {0}\".format(path2dic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeGdqQyk_1J8"
      },
      "outputs": [],
      "source": [
        "def bunsetsu_wakachi(text):\n",
        "    mecab_result = mecab.parse(text).splitlines()\n",
        "    mecab_result = mecab_result[:-1] #最後の1行は不要な行なので除く\n",
        "    break_pos = ['名詞','動詞','接頭詞','副詞','感動詞','形容詞','形容動詞','連体詞'] #文節の切れ目を検出するための品詞リスト\n",
        "    wakachi = [''] #分かち書きのリスト\n",
        "    prev_pos_detail = []\n",
        "    afterPrepos = False #接頭詞の直後かどうかのフラグ\n",
        "    afterSahenNoun = False #サ変接続名詞の直後かどうかのフラグ\n",
        "    nextNoBreak = False\n",
        "\n",
        "    for v in mecab_result:\n",
        "        if '\\t' not in v: continue\n",
        "        surface = v.split('\\t')[0] #表層系\n",
        "        pos = v.split('\\t')[1].split(',') #品詞など\n",
        "        pos_detail = ','.join(pos[1:4]) #品詞細分類（各要素の内部がさらに'/'で区切られていることがあるので、','でjoinして、inで判定する)\n",
        "\n",
        "        #この単語が文節の切れ目とならないかどうかの判定\n",
        "        noBreak = False\n",
        "        if nextNoBreak:\n",
        "          noBreak = True\n",
        "\n",
        "        nextNoBreak = False\n",
        "        noBreak = noBreak or (pos[0] not in break_pos)\n",
        "        noBreak = noBreak or ('数' in prev_pos_detail and '数' in pos_detail) # 数が連続する場合は文節の切れ目としない\n",
        "        noBreak = noBreak or '接尾' in pos_detail\n",
        "        noBreak = noBreak or (pos[0]=='動詞' and 'サ変接続' in pos_detail)\n",
        "        noBreak = noBreak or '非自立' in pos_detail #非自立な名詞、動詞を文節の切れ目としたい場合はこの行をコメントアウトする\n",
        "        noBreak = noBreak or afterPrepos\n",
        "        noBreak = noBreak or (afterSahenNoun and pos[0]=='動詞' and pos[4]=='サ変・スル')\n",
        "\n",
        "        if surface.endswith('-'):\n",
        "          noBreak = True\n",
        "          nextNoBreak = True\n",
        "\n",
        "        if noBreak == False:\n",
        "            wakachi.append(\"\") # 要素を増やす（つまり次の文節に行く）\n",
        "\n",
        "        if not nextNoBreak: # '-' 以外は加える\n",
        "          wakachi[-1] += surface # 要素は増やさず最後のやつに加える（まだ文節であるとして繋げる）\n",
        "\n",
        "        afterPrepos = pos[0]=='接頭詞'\n",
        "        afterSahenNoun = 'サ変接続' in pos_detail\n",
        "        prev_pos_detail = pos_detail\n",
        "\n",
        "    if wakachi[0] == '': wakachi = wakachi[1:] #最初が空文字のとき削除する\n",
        "    return wakachi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCslyOr_8u5"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k2Ki9wz_4Sd"
      },
      "outputs": [],
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install fugashi ipadic\n",
        "!pip install datasets unidic-lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUKYYIskmUQf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertJapaneseTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2drxSv66_lat"
      },
      "outputs": [],
      "source": [
        "# モデルとトークナイザの設定\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n",
        "model = BertForMaskedLM.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n",
        "model = model.to('cuda:0')\n",
        "\n",
        "# トークンIDの設定\n",
        "tokenizer.add_tokens('[NONE]') # 特殊トークン [NONE] をトークナイザーに追加\n",
        "model.resize_token_embeddings(len(tokenizer)) # モデルにも新しいトークンを追加\n",
        "none_token_id = tokenizer.convert_tokens_to_ids('[NONE]') # トークンIDを取得\n",
        "comma_id = tokenizer.convert_tokens_to_ids('、')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_JOuKHfNmcU"
      },
      "source": [
        "# 各種関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgO3GgoMBpXq"
      },
      "outputs": [],
      "source": [
        "def get_texts(data_path):\n",
        "  texts = []\n",
        "  with open(data_path, encoding='cp932') as file:\n",
        "    for line in file:\n",
        "      if 511 < len(line):\n",
        "        while 511 < len(line):\n",
        "          first_256_chars = line[:256] # 先頭から256文字を取得\n",
        "          last_period_index = first_256_chars.rfind(\"。\") # 取得した部分から最後の \"。\" までの位置を検索\n",
        "          # 最後の \"。\" までの部分を別の変数に代入\n",
        "          if last_period_index != -1:\n",
        "            extracted_part = first_256_chars[:last_period_index + 1]\n",
        "            texts.append(extracted_part)\n",
        "            line = line[len(extracted_part):] # もとのtextから抽出した部分を除いた部分を更新\n",
        "          else:\n",
        "            print(\"先頭から256文字までに 。 がない\")\n",
        "            break\n",
        "\n",
        "      line = line.replace(\"\\n\", \"\")\n",
        "      sentence_list = line.split(\"。\") # この時点で、\"。\" はなくなる\n",
        "      sentences = []\n",
        "      for s in sentence_list:\n",
        "        if len(s) > 0: sentences.append(s + \"。\") # \"。\"　を復元する\n",
        "      if 0 < len(sentences): texts += sentences\n",
        "\n",
        "  return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKlN5pWFBvt6"
      },
      "outputs": [],
      "source": [
        "# 入力：読点を除去したテキスト / 出力：文節境界に [MASK] を挿入している\n",
        "def insert_masks_between_bunsetsu(text):\n",
        "  bunsetsus = bunsetsu_wakachi(text)\n",
        "  masked_text = ''\n",
        "  for bunsetsu in bunsetsus:\n",
        "    masked_text += bunsetsu\n",
        "    if not bunsetsu.endswith(\"、\"):\n",
        "      masked_text += '[MASK]'\n",
        "  masked_text = masked_text[:-6] # 末尾の余分な '[MASK]' を削除\n",
        "  return masked_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pzhZmchQLsK"
      },
      "outputs": [],
      "source": [
        "def get_is_comma(original_text, masked_text):\n",
        "    result = \"\"\n",
        "    i = j = 0\n",
        "    while i < len(original_text) and j < len(masked_text):\n",
        "      if masked_text[j] == '[':\n",
        "        if original_text[i] == '、':\n",
        "          return True\n",
        "        else:\n",
        "          return False\n",
        "      elif original_text[i] == '、': # 文字列 A に読点があり、文字列 B に読点がない場合\n",
        "        i += 1\n",
        "      else: # 両方の文字が一致する場合\n",
        "        i += 1\n",
        "        j += 1\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_masked_texts_and_is_commas(original_text):\n",
        "  removed_text = original_text.replace(\"、\", \"\")\n",
        "  masked_text = insert_masks_between_bunsetsu(removed_text)\n",
        "\n",
        "  masked_texts = []\n",
        "  is_commas = []\n",
        "\n",
        "  while \"[MASK]\" in masked_text:\n",
        "    # target_masked_text は、masked_text の最初の [MASK] だけを残したもの\n",
        "    first_mask_index = masked_text.find(\"[MASK]\")\n",
        "    target_masked_text = masked_text[:first_mask_index + 6] + masked_text[first_mask_index + 6:].replace(\"[MASK]\", \"\")\n",
        "    is_comma = get_is_comma(original_text, target_masked_text)\n",
        "\n",
        "    masked_texts.append(target_masked_text) # 追加\n",
        "    is_commas.append(is_comma) # 追加\n",
        "    masked_text = masked_text.replace(\"[MASK]\", \"\", 1) # 最初の [MASK] を消す（その [MASK] は完了）\n",
        "\n",
        "  return [masked_texts, is_commas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR5XtN5TBY6w"
      },
      "source": [
        "# ファインチューニング"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_xAZyjAJxqI"
      },
      "outputs": [],
      "source": [
        "class JapaneseMLMDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        label_ids = [-100] * len(item[\"input_ids\"])\n",
        "        mask_idx = torch.where(item[\"input_ids\"] == tokenizer.mask_token_id)[0]\n",
        "        label_ids[mask_idx] = comma_id if self.labels[idx] else none_token_id\n",
        "        item['labels'] = torch.tensor(label_ids)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "def finetune(model, texts):\n",
        "  masked_texts = []\n",
        "  labels = []\n",
        "  for i in tqdm(range(len(texts))):\n",
        "    original_text = texts[i - 1]\n",
        "    masked_texts_and_is_commas = get_masked_texts_and_is_commas(original_text)\n",
        "    if len(masked_texts_and_is_commas) == 2:\n",
        "      masked_texts += masked_texts_and_is_commas[0]\n",
        "      labels += masked_texts_and_is_commas[1]\n",
        "\n",
        "  # データセットのトークナイズ\n",
        "  encodings = tokenizer(masked_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "  dataset = JapaneseMLMDataset(encodings, labels)\n",
        "\n",
        "  # トレーニングの設定\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir='./results',\n",
        "      num_train_epochs=3,\n",
        "      per_device_train_batch_size=16,\n",
        "      warmup_steps=500,\n",
        "      weight_decay=0.01,\n",
        "      logging_dir='./logs',\n",
        "  )\n",
        "\n",
        "  # トレーナーの初期化\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=dataset,\n",
        "  )\n",
        "\n",
        "  # ファインチューニングの実行\n",
        "  trainer.train()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqSuQ1_fWHXh"
      },
      "source": [
        "# 読点挿入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfFcfu04srXr"
      },
      "outputs": [],
      "source": [
        "def insert_commas(model, texts):\n",
        "  masked_text = insert_masks_between_bunsetsu(text)\n",
        "\n",
        "  prev_token = \"\"\n",
        "  while \"[MASK]\" in masked_text:\n",
        "    # target_masked_text は、masked_text の最初の [MASK] だけを残したもの\n",
        "    first_mask_index = masked_text.find(\"[MASK]\")\n",
        "    target_masked_text = masked_text[:first_mask_index + 6] + masked_text[first_mask_index + 6:].replace(\"[MASK]\", \"\")\n",
        "\n",
        "    input_ids = tokenizer.encode(target_masked_text, return_tensors='pt')   # トークナイズしてテンソルに変換する\n",
        "    input_ids = input_ids.to('cuda:0')  # GPU\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids) # モデルに入力して予測する\n",
        "    predictions = output[0][0] # 予測結果をトークンに戻す\n",
        "\n",
        "    input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    for i, prediction in enumerate(predictions): # 各トークンに対するインデックスと予測\n",
        "      if input_ids[0][i] == tokenizer.mask_token_id: # [MASK] トークンの場所以外に興味なし\n",
        "        score = 1\n",
        "        comma_value = prediction[comma_id]\n",
        "        none_token_value = prediction[none_token_id]\n",
        "\n",
        "        if comma_value > none_token_value\n",
        "          masked_text = masked_text.replace(\"[MASK]\", \"、\", 1) # 最初の [MASK] を置き換える\n",
        "        else:\n",
        "          masked_text = masked_text.replace(\"[MASK]\", \"\", 1) # 最初の [MASK] を置き換える\n",
        "\n",
        "      prev_token = input_tokens[i]\n",
        "\n",
        "  punctuated_text = masked_text\n",
        "  punctuated_text = re.sub(\"#|\\[CLS]|\\[SEP]\", \"\", punctuated_text)\n",
        "  return punctuated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkN86Np6W8Jg"
      },
      "source": [
        "# 挿入結果の評価"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsRaR7aGAGrS"
      },
      "outputs": [],
      "source": [
        "# テキストに含まれる読点のインデックスのリストを返す\n",
        "def get_comma_indexes(text):\n",
        "  comma_indexes = []\n",
        "  for i in range(len(text)):\n",
        "      if text[i] == \"、\":\n",
        "          comma_indexes.append(i)\n",
        "  return comma_indexes\n",
        "\n",
        "def calculate_result(original_text, output_text):\n",
        "  # 読点のインデックスのリストを取得\n",
        "  original_indexes = get_comma_indexes(original_text)\n",
        "  output_indexes = get_comma_indexes(output_text)\n",
        "  saigen = 0 # 再現できている読点の数 （saigen / 挿入した数 で 適合率 も出せる）\n",
        "  num_original_indexes = len(original_indexes)\n",
        "  num_output_indexes = len(output_indexes)\n",
        "\n",
        "  # 原文または出力文のどちらかの読点数がゼロなら saigen == 0 なので終わる\n",
        "  if num_original_indexes == 0 or num_output_indexes == 0:\n",
        "    return [original_text, output_text, num_original_indexes, num_output_indexes, saigen]\n",
        "\n",
        "  # 再現数のカウント\n",
        "  output_i = 0\n",
        "  index_diff = 0\n",
        "\n",
        "  for i in range(num_original_indexes):\n",
        "    original_index = original_indexes[i]+index_diff\n",
        "    output_index = output_indexes[output_i]\n",
        "\n",
        "    if original_index == output_index:\n",
        "      # output_index で再現できている場合\n",
        "      saigen += 1\n",
        "      if output_i+1 < num_output_indexes:\n",
        "        output_i += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    elif original_index > output_index:\n",
        "      # 原文の読点までに、余分に挿入した読点がある場合\n",
        "      while original_indexes[i]+index_diff > output_indexes[output_i]:\n",
        "        if output_i+1 < num_output_indexes:\n",
        "          output_i += 1\n",
        "          index_diff += 1\n",
        "        else:\n",
        "          break\n",
        "      if original_indexes[i]+index_diff == output_indexes[output_i]:\n",
        "        # output_index で再現できている場合\n",
        "        saigen += 1\n",
        "        if output_i+1 < num_output_indexes:\n",
        "          output_i += 1\n",
        "        else:\n",
        "          break\n",
        "      else:\n",
        "        # i番目の読点は再現できていないことが確定\n",
        "        index_diff -= 1\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "      # i番目の読点は再現できていないことが確定\n",
        "      index_diff -= 1\n",
        "      continue\n",
        "\n",
        "  return [original_text, output_text, num_original_indexes, num_output_indexes, saigen]\n",
        "\n",
        "def print_result(results):\n",
        "  num_all_saigen = 0\n",
        "  num_all_original_commas = 0\n",
        "  num_all_output_commas = 0\n",
        "\n",
        "  for result in results:\n",
        "    # print(\"原文： \" + result[0])\n",
        "    # print(\"出力： \" + result[1])\n",
        "    num_saigen = result[4]\n",
        "    num_original_commas = result[2]\n",
        "    num_output_commas = result[3]\n",
        "\n",
        "    # 統計に追加\n",
        "    num_all_saigen += num_saigen\n",
        "    num_all_original_commas += num_original_commas\n",
        "    num_all_output_commas += num_output_commas\n",
        "    # print(\"------------------------\")\n",
        "\n",
        "    # 指標を計算\n",
        "    precision = 0\n",
        "    if num_all_output_commas > 0:\n",
        "      precision = num_all_saigen / num_all_output_commas # 適合率\n",
        "    recall = 0\n",
        "    if num_all_original_commas > 0:\n",
        "      recall = num_all_saigen / num_all_original_commas # 再現率\n",
        "    f_value = 0\n",
        "    if (precision + recall) > 0:\n",
        "      f_value = 2 * precision * recall / (precision + recall) # F値\n",
        "\n",
        "  if num_all_original_commas != 0 and num_all_output_commas != 0:\n",
        "    print(f\"再現率: {recall: .3f}, 適合率: {precision: .3f}, F値： {f_value: .3f}\")\n",
        "  else:\n",
        "    print(f\"再現率: 0, 適合率: 0, F値： {f_value: .3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfY8tTlfXGOS"
      },
      "source": [
        "# テスト"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmrUbM8yyZ-e"
      },
      "outputs": [],
      "source": [
        "train_data_path = '/content/drive/My Drive/Colab Notebooks/TextData/TrainData.txt'\n",
        "test_data_path = '/content/drive/My Drive/Colab Notebooks/TextData/TestData.txt'\n",
        "\n",
        "# 読点位置の学習\n",
        "texts = get_texts(train_data_path)\n",
        "finetuned_model = finetune(model, texts)\n",
        "\n",
        "# 読点挿入\n",
        "texts = get_texts(test_data_path)\n",
        "for i in tqdm(range(len(texts))):\n",
        "  text = texts[i - 1]\n",
        "  removed_text = re.sub(\"、\", \"\", text)\n",
        "  output_text = \"\"\n",
        "  output_text = insert_commas(finetuned_model, texts)\n",
        "  results.append(calculate_result(text, output_text))\n",
        "\n",
        "print_result(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iqnQkmL1X8HJ",
        "Gy6saF4Qg08H",
        "wR5XtN5TBY6w",
        "hkN86Np6W8Jg",
        "TfY8tTlfXGOS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
